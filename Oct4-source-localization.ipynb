{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] 找不到指定的模块。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ee7511abbe76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultioutput\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultiOutputRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mXGBR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m  \u001b[0mpandas\u001b[0m  \u001b[1;32mas\u001b[0m  \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\scipy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;31m# Allow distributors to run custom init code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pep440\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\scipy\\_distributor_init.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibs_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibs_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'*dll'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m                 \u001b[0mWinDLL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mowd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\ctypes\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] 找不到指定的模块。"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import graphviz\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor as XGBR\n",
    "import  pandas  as  pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold,train_test_split,GridSearchCV, cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score,train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import RFECV\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import plot_tree\n",
    "from time import *\n",
    "import os\n",
    "import cmath as ch\n",
    "import scipy.io\n",
    "import heapq\n",
    "import hyperopt\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "begin_time = time() # Calculate training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = './datafile.mat'\n",
    "data_info = scipy.io.loadmat(source_data)\n",
    "data = data_info['obs_data']\n",
    "sim_data = data_info['sim_data']\n",
    "Sample_amount = sim_data.shape[0]\n",
    "Scaling_factor = data_info['Scaling_factor']\n",
    "release_loc = data_info['release_loc']\n",
    "\n",
    "start_time = 11\n",
    "end_time = 41\n",
    "interval = end_time - start_time\n",
    "percentile = 0.4\n",
    "J = []\n",
    "measure1 = 0\n",
    "measure2 = 0\n",
    "measure3 = 0\n",
    "measure4 = 0\n",
    "measure = []\n",
    "for k in range(start_time,end_time):\n",
    "    measure1 = measure1+data[k,0]\n",
    "    measure2 = measure2+data[k,1]\n",
    "    measure3 = measure3+data[k,2]\n",
    "    measure4 = measure4+data[k,3]\n",
    "measure_aver1 = measure1 / interval #测点1测量值的平均值\n",
    "measure.append(measure_aver1)\n",
    "measure_aver2 = measure2 / interval #测点2测量值的平均值\n",
    "measure.append(measure_aver2)\n",
    "measure_aver3 = measure3 / interval #测点3测量值的平均值\n",
    "measure.append(measure_aver3)\n",
    "measure_aver4 = measure4 / interval #测点4测量值的平均值\n",
    "measure.append(measure_aver4)\n",
    "measure_aver = (measure_aver1+measure_aver2+measure_aver3+measure_aver4)/4\n",
    "\n",
    "#读取释放点的模拟数据\n",
    "for j in range(Sample_amount):\n",
    "    Oct4_sim = [\n",
    "                    np.mean(sim_data[j,start_time:end_time,0]),\n",
    "                    np.mean(sim_data[j,start_time:end_time,1]),\n",
    "                    np.mean(sim_data[j,start_time:end_time,2]),\n",
    "                    np.mean(sim_data[j,start_time:end_time,3])\n",
    "               ]\n",
    "    \n",
    "    Oct4_sim_aver = np.mean(Oct4_sim)\n",
    "    sum1_1=0\n",
    "    sum1_2=0\n",
    "    sum1_3=0\n",
    "    for i in range(0,4):\n",
    "        measure_bias = measure[i] - measure_aver\n",
    "        measure_var = measure_bias ** 2\n",
    "        Oct4_sim_bias = Oct4_sim[i] - Oct4_sim_aver\n",
    "        Oct4_sim_var = Oct4_sim_bias ** 2\n",
    "        temp1_1 = measure_bias * Oct4_sim_bias\n",
    "        sum1_1 = sum1_1 + temp1_1\n",
    "        sum1_2 = sum1_2 + Oct4_sim_var\n",
    "        sum1_3 = sum1_3 + measure_var\n",
    "    aver1_1 = sum1_1/4\n",
    "    aver1_2 = np.sqrt(sum1_2/4)\n",
    "    aver1_3 = np.sqrt(sum1_3/4) \n",
    "    J1_temp = aver1_1/(aver1_2*aver1_3)\n",
    "    J.append(J1_temp)\n",
    "\n",
    "index = J.index(max(J))\n",
    "\n",
    "def OperCount(data,number,operator):\n",
    "    num = 0\n",
    "    if operator == '>':\n",
    "        for i in data:\n",
    "            if i > number:\n",
    "                num+=1\n",
    "    elif operator == '<':\n",
    "        for i in data:\n",
    "            if i < number:\n",
    "                num+=1\n",
    "    elif operator == '=':\n",
    "        for i in data:\n",
    "            if i == number:\n",
    "                num+=1\n",
    "    else:\n",
    "        print('something wrong!')\n",
    "    percent = num/len(data)\n",
    "    return num,percent\n",
    "\n",
    "J = list(J)\n",
    "index_train = map(J.index,heapq.nlargest(int(Sample_amount*percentile),J))\n",
    "index_train = list(index_train)\n",
    "\n",
    "J = pd.DataFrame(J)\n",
    "writer = pd.ExcelWriter('./Reconstruction_results/cost_function_Oct4.xls')\n",
    "J.to_excel(writer,header=False,index=False)\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train = np.array(index_train)\n",
    "index_train = index_train.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./Training_datasets/train_data.csv\",index_col=0) # 读取特征数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[index_train,:]\n",
    "data.info()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feature = data.drop([\"x\",\"y\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_target = data.iloc[:,24:26]\n",
    "data_target_x = data.iloc[:,24:25]\n",
    "data_target_y = data.iloc[:,25:26]\n",
    "data_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(data_feature,data_target,test_size=0.3,random_state=320)\n",
    "for i in [Xtrain, Xtest, Ytrain, Ytest]:\n",
    "    i.index = range(i.shape[0]) #将索引转化成0-shape[0]的整数，就可以恢复索引\n",
    "\n",
    "Xtrain_x, Xtest_x, Ytrain_x, Ytest_x = train_test_split(data_feature,data_target_x,test_size=0.3,random_state=320)\n",
    "for i in [Xtrain_x, Xtest_x, Ytrain_x, Ytest_x]:\n",
    "    i.index = range(i.shape[0]) #将索引转化成0-shape[0]的整数，就可以恢复索引\n",
    " \n",
    "Xtrain_y, Xtest_y, Ytrain_y, Ytest_y = train_test_split(data_feature,data_target_y,test_size=0.3,random_state=320)\n",
    "for i in [Xtrain_y, Xtest_y, Ytrain_y, Ytest_y]:\n",
    "    i.index = range(i.shape[0]) #将索引转化成0-shape[0]的整数，就可以恢复索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCVA = []\n",
    "MCVA_x = []\n",
    "MCVA_y = []\n",
    "\n",
    "GC = []\n",
    "GC_x = []\n",
    "GC_y = []\n",
    "TS = []\n",
    "LE = []\n",
    "x_estimate = []\n",
    "y_estimate = []\n",
    "n_estimators = []\n",
    "learning_rate = []\n",
    "max_depth = []\n",
    "gamma = []\n",
    "subsample = []\n",
    "reg_lambda = []\n",
    "colsample_bytree = []\n",
    "min_child_weight = []\n",
    "\n",
    "for i in range(50):\n",
    "    print('Loop time：',i)\n",
    "    def huber_approx_obj(real,predict):\n",
    "        d = predict -real\n",
    "        h = 2 #h is delta in the formula\n",
    "        scale = 1 + (d / h) ** 2\n",
    "        scale_sqrt = np.sqrt(scale)\n",
    "        grad = d / scale_sqrt\n",
    "        hess = 1 / scale / scale_sqrt\n",
    "        return grad, hess\n",
    "\n",
    "    space={ 'n_estimators': hp.uniform ('n_estimators', 50,300),\n",
    "            'learning_rate': hp.uniform('learning_rate', 0.05, 0.3),\n",
    "            'max_depth': hp.quniform(\"max_depth\", 3, 8, 1),\n",
    "            'gamma': hp.uniform('gamma', 0.01,1),\n",
    "            'subsample' : hp.uniform('subsample', 0.5,1),\n",
    "            'reg_lambda' : hp.uniform('reg_lambda', 0.01,5),\n",
    "            'colsample_bytree' : hp.uniform('colsample_bytree', 0.01,1),\n",
    "            'min_child_weight' : hp.quniform('min_child_weight', 2, 10, 1),   \n",
    "        }\n",
    "\n",
    "    def hyperparameter_tuning(space):\n",
    "        param = {\n",
    "                 'n_estimators':     int(space['n_estimators']), \n",
    "                 'learning_rate':    space['learning_rate'], \n",
    "                 'max_depth':        int(space['max_depth']),\n",
    "                 'gamma':            space['gamma'], \n",
    "                 'subsample':        space['subsample'],\n",
    "                 'reg_lambda':       space['reg_lambda'],\n",
    "                 'colsample_bytree': space['colsample_bytree'],\n",
    "                 'min_child_weight': space['min_child_weight'],  \n",
    "                 \"random_state\":123456\n",
    "                }\n",
    "        model = MultiOutputRegressor(XGBR(objective='reg:squarederror',**param)) #objective='reg:squarederror'\n",
    "        r2 = cross_val_score(model,data_feature,data_target,cv=5) #scoring='neg_mean_squared_error'\n",
    "        return (1-r2.mean())**2+r2.var()\n",
    "    from hyperopt import tpe\n",
    "    tpe_algorithm = tpe.suggest\n",
    "    from hyperopt import fmin\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=hyperparameter_tuning,\n",
    "                space=space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=100,\n",
    "                trials=trials)\n",
    "\n",
    "\n",
    "    print (best)\n",
    "    end_time = time()\n",
    "\n",
    "    run_time = end_time-begin_time\n",
    "\n",
    "    print(run_time)\n",
    "    \n",
    "    learning_rate.append(best['learning_rate'])\n",
    "    max_depth.append(int(best['max_depth']))\n",
    "    n_estimators.append(int(best['n_estimators']))\n",
    "    min_child_weight.append(best['min_child_weight'])\n",
    "    subsample.append(best['subsample'])\n",
    "    colsample_bytree.append(best['colsample_bytree'])\n",
    "    reg_lambda.append(best['reg_lambda'])\n",
    "    gamma.append(best['gamma'])\n",
    "    \n",
    "    param = {\n",
    "                 'max_depth':        int(best['max_depth']),\n",
    "                 'learning_rate':    best['learning_rate'],\n",
    "                 'n_estimators':     int(best['n_estimators']),  \n",
    "                 'min_child_weight': best['min_child_weight'],\n",
    "                 'subsample':        best['subsample'],\n",
    "                 'colsample_bytree': best['colsample_bytree'],\n",
    "                 'reg_lambda':       best['reg_lambda'],\n",
    "                 'gamma':            best['gamma'], \n",
    "                 \"random_state\":123456\n",
    "                }\n",
    "    x_xgbr = XGBR(objective='reg:squarederror',**param)\n",
    "    scores_x = cross_val_score(x_xgbr,data_feature,data_target_x,cv=5)\n",
    "    MCVA_x_temp = scores_x.mean()\n",
    "    MCVA_x_temp = np.around(MCVA_x_temp, decimals = 4)\n",
    "    MCVA_x.append(MCVA_x_temp)\n",
    "    \n",
    "    GC_x_temp = (1-scores_x.mean())**2+scores_x.var()\n",
    "    GC_x_temp = np.around(GC_x_temp, decimals = 4)\n",
    "    GC_x.append(GC_x_temp)\n",
    "    \n",
    "    x_xgbr.fit(Xtrain_x,Ytrain_x)\n",
    "\n",
    "    y_xgbr = XGBR(objective='reg:squarederror',**param)\n",
    "    scores_y = cross_val_score(y_xgbr,data_feature,data_target_y,cv=5)\n",
    "    MCVA_y_temp = scores_y.mean()\n",
    "    MCVA_y_temp = np.around(MCVA_y_temp, decimals = 4)\n",
    "    MCVA_y.append(MCVA_y_temp)\n",
    "    \n",
    "    GC_y_temp = (1-scores_y.mean())**2+scores_y.var()\n",
    "    GC_y_temp = np.around(GC_y_temp, decimals = 4)\n",
    "    GC_y.append(GC_y_temp)\n",
    "    \n",
    "    y_xgbr.fit(Xtrain_y,Ytrain_y)\n",
    "\n",
    "    multioutputregressor = MultiOutputRegressor(XGBR(objective='reg:squarederror',**param))\n",
    "    scores = cross_val_score(multioutputregressor,data_feature,data_target,cv=5)\n",
    "    MCVA_temp = scores.mean()\n",
    "    MCVA_temp = np.around(MCVA_temp, decimals = 4)\n",
    "    MCVA.append(MCVA_temp)\n",
    "    \n",
    "    GC_temp = (1-scores.mean())**2+scores.var()\n",
    "    GC_temp = np.around(GC_temp, decimals = 4)\n",
    "    GC.append(GC_temp)\n",
    "    \n",
    "    multioutputregressor.fit(Xtrain, Ytrain)\n",
    "\n",
    "    print('Prediction result: x, y')\n",
    "    TS_temp = multioutputregressor.score(Xtest,Ytest)\n",
    "    TS_temp = np.around(TS_temp, decimals = 4)\n",
    "    TS.append(TS_temp)\n",
    "    \n",
    "    source = pd.read_csv(\"./Testing_datasets/test_data.csv\",index_col=0) # 12-40\n",
    "\n",
    "\n",
    "    Multi_pred_source=multioutputregressor.predict(source)\n",
    "    print(Multi_pred_source)\n",
    "    Multi_pred_source_x=Multi_pred_source[0,0]\n",
    "    Multi_pred_source_x = np.around(Multi_pred_source_x,decimals=4)\n",
    "    x_estimate.append(Multi_pred_source_x)\n",
    "    Multi_pred_source_y=Multi_pred_source[0,1]\n",
    "    Multi_pred_source_y = np.around(Multi_pred_source_y,decimals=4)\n",
    "    y_estimate.append(Multi_pred_source_y)\n",
    "    LE_temp = np.sqrt((Multi_pred_source_x - 4565)**2+(Multi_pred_source_y - 7621)**2)\n",
    "    LE_temp = np.around(LE_temp, decimals = 4)\n",
    "    LE.append(LE_temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Inversion_results = {'MCVA':MCVA,\n",
    "                    'MCVA_x':MCVA_x,\n",
    "                    'MCVA_y':MCVA_y,\n",
    "                    'GC':GC,\n",
    "                    'GC_x':GC_x,\n",
    "                    'GC_y':GC_y,\n",
    "                    'TS': TS,\n",
    "                    'x':x_estimate,\n",
    "                    'y':y_estimate,\n",
    "                    'LE':LE,\n",
    "                    'max_depth':max_depth,\n",
    "                    'learning_rate':learning_rate,\n",
    "                    'n_estimators':n_estimators,\n",
    "                    'min_child_weight':min_child_weight,\n",
    "                    'subsample':subsample,\n",
    "                    'colsample_bytree':colsample_bytree,\n",
    "                    'reg_lambda':reg_lambda,\n",
    "                    'gamma':gamma\n",
    "                    }\n",
    "Inversion_results['x'] = np.around(Inversion_results['x'],decimals=4)\n",
    "Inversion_results['y'] = np.around(Inversion_results['y'],decimals=4)\n",
    "Inversion_results = pd.DataFrame(Inversion_results)\n",
    "Inversion_results.to_csv('./Reconstruction_results/inversion_Oct4_before_feature_selection_50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面进行特征筛选\n",
    "Xtrain_x, Xtest_x, Ytrain_x, Ytest_x = train_test_split(data_feature,data_target_x,test_size=0.3,random_state=320)\n",
    "for i in [Xtrain_x, Xtest_x, Ytrain_x, Ytest_x]:\n",
    "    i.index = range(i.shape[0]) #将索引转化成0-shape[0]的整数，就可以恢复索引\n",
    "# 查看分好的训练集和测试集\n",
    "\n",
    "Xtrain_y, Xtest_y, Ytrain_y, Ytest_y = train_test_split(data_feature,data_target_y,test_size=0.3,random_state=320)\n",
    "for i in [Xtrain_y, Xtest_y, Ytrain_y, Ytest_y]:\n",
    "    i.index = range(i.shape[0]) #将索引转化成0-shape[0]的整数，就可以恢复索引\n",
    "#Xtrain.head()\n",
    "\n",
    "other_params = {\n",
    "                 'max_depth':        int(best['max_depth']),\n",
    "                 'learning_rate':    best['learning_rate'],\n",
    "                 'n_estimators':     int(best['n_estimators']),  \n",
    "                 'min_child_weight': best['min_child_weight'],\n",
    "                 'subsample':        best['subsample'],\n",
    "                 'colsample_bytree': best['colsample_bytree'],\n",
    "                 'reg_lambda':       best['reg_lambda'],\n",
    "                 'gamma':            best['gamma'], \n",
    "                 \"random_state\":123456\n",
    "}\n",
    "\n",
    "x_xgbr = XGBR(objective='reg:squarederror',**other_params)\n",
    "y_xgbr = XGBR(objective='reg:squarederror',**other_params)\n",
    "\n",
    "\n",
    "selectorx = RFECV(x_xgbr,cv=5,step=1).fit(Xtrain_x,Ytrain_x)\n",
    "selectory = RFECV(y_xgbr,cv=5,step=1).fit(Xtrain_y,Ytrain_y)\n",
    "#check=multioutputregressor.predict(Xtest)\n",
    "#multioutputregressor.score(Xtest,Ytest)#默认为Return the coefficient of determination R^2 of the prediction.\n",
    "\n",
    "print(selectorx.support_.sum())\n",
    "print(selectorx.ranking_)\n",
    "idx=[]\n",
    "for i in range(24):\n",
    "    if selectorx.ranking_[i]!=1:\n",
    "        idx.append(i)\n",
    "print(idx)\n",
    "X_wrapper_x=selectorx.transform(Xtrain_x)\n",
    "cross_val_score(x_xgbr,X_wrapper_x,Ytrain_x,cv=5).mean()\n",
    "\n",
    "print(selectory.support_.sum())\n",
    "print(selectory.ranking_)\n",
    "idy=[]\n",
    "for i in range(24):\n",
    "    if selectory.ranking_[i]!=1:\n",
    "        idy.append(i)\n",
    "print(idy)\n",
    "X_wrapper_y=selectory.transform(Xtrain_y)\n",
    "cross_val_score(y_xgbr,X_wrapper_y,Ytrain_y,cv=5).mean()\n",
    "\n",
    "\n",
    "id_delete = list(set(idx).intersection(set(idy)))\n",
    "\n",
    "data_feature_all = ['wave_rate1','aver1','median1','fft_shape_mean1','fft_mean1','SamEn1',\n",
    "                   'wave_rate2','aver2','median2','fft_shape_mean2','fft_mean2','SamEn2',\n",
    "                   'wave_rate3','aver3','median3','fft_shape_mean3','fft_mean3','SamEn3',\n",
    "                   'wave_rate4','aver4','median4','fft_shape_mean4','fft_mean4','SamEn4',]\n",
    "data_feature = data.drop([\"x\",\"y\"],axis=1)\n",
    "for i in range(len(id_delete)):\n",
    "    data_feature = data_feature.drop(data_feature_all[id_delete[i]],axis=1)\n",
    "print(data_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(data_feature,data_target,test_size=0.3,random_state=320)\n",
    "for i in [Xtrain, Xtest, Ytrain, Ytest]:\n",
    "    i.index = range(i.shape[0]) #将索引转化成0-shape[0]的整数，就可以恢复索引\n",
    "\n",
    "Xtrain_x, Xtest_x, Ytrain_x, Ytest_x = train_test_split(data_feature,data_target_x,test_size=0.3,random_state=320)\n",
    "for i in [Xtrain_x, Xtest_x, Ytrain_x, Ytest_x]:\n",
    "    i.index = range(i.shape[0]) #将索引转化成0-shape[0]的整数，就可以恢复索引\n",
    " \n",
    "Xtrain_y, Xtest_y, Ytrain_y, Ytest_y = train_test_split(data_feature,data_target_y,test_size=0.3,random_state=320)\n",
    "for i in [Xtrain_y, Xtest_y, Ytrain_y, Ytest_y]:\n",
    "    i.index = range(i.shape[0]) #将索引转化成0-shape[0]的整数，就可以恢复索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCVA = []\n",
    "MCVA_x = []\n",
    "MCVA_y = []\n",
    "\n",
    "GC = []\n",
    "GC_x = []\n",
    "GC_y = []\n",
    "TS = []\n",
    "LE = []\n",
    "x_estimate = []\n",
    "y_estimate = []\n",
    "x_estimate = []\n",
    "y_estimate = []\n",
    "n_estimators = []\n",
    "learning_rate = []\n",
    "max_depth = []\n",
    "gamma = []\n",
    "subsample = []\n",
    "reg_lambda = []\n",
    "colsample_bytree = []\n",
    "min_child_weight = []\n",
    "\n",
    "for i in range(50):\n",
    "    print('Loop time：',i)\n",
    "    def huber_approx_obj(real,predict):\n",
    "        d = predict -real\n",
    "        h = 2 #h is delta in the formula\n",
    "        scale = 1 + (d / h) ** 2\n",
    "        scale_sqrt = np.sqrt(scale)\n",
    "        grad = d / scale_sqrt\n",
    "        hess = 1 / scale / scale_sqrt\n",
    "        return grad, hess\n",
    "\n",
    "    space={ 'n_estimators': hp.uniform ('n_estimators', 50,300),\n",
    "            'learning_rate': hp.uniform('learning_rate', 0.05, 0.3),\n",
    "            'max_depth': hp.quniform(\"max_depth\", 3, 8, 1),\n",
    "            'gamma': hp.uniform('gamma', 0.01,1),\n",
    "            'subsample' : hp.uniform('subsample', 0.5,1),\n",
    "            'reg_lambda' : hp.uniform('reg_lambda', 0.01,5),\n",
    "            'colsample_bytree' : hp.uniform('colsample_bytree', 0.01,1),\n",
    "            'min_child_weight' : hp.quniform('min_child_weight', 2, 10, 1),   \n",
    "        }\n",
    "\n",
    "    def hyperparameter_tuning(space):\n",
    "        param = {\n",
    "                 'n_estimators':     int(space['n_estimators']), \n",
    "                 'learning_rate':    space['learning_rate'], \n",
    "                 'max_depth':        int(space['max_depth']),\n",
    "                 'gamma':            space['gamma'], \n",
    "                 'subsample':        space['subsample'],\n",
    "                 'reg_lambda':       space['reg_lambda'],\n",
    "                 'colsample_bytree': space['colsample_bytree'],\n",
    "                 'min_child_weight': space['min_child_weight'],  \n",
    "                 \"random_state\":123456\n",
    "                }\n",
    "        model = MultiOutputRegressor(XGBR(objective='reg:squarederror',**param)) #objective='reg:squarederror'\n",
    "        r2 = cross_val_score(model,data_feature,data_target,cv=5) #scoring='neg_mean_squared_error'\n",
    "        return (1-r2.mean())**2+r2.var()\n",
    "\n",
    "    from hyperopt import tpe\n",
    "    tpe_algorithm = tpe.suggest\n",
    "    from hyperopt import fmin\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=hyperparameter_tuning,\n",
    "                space=space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=100,\n",
    "                trials=trials)\n",
    "\n",
    "\n",
    "    print (best)\n",
    "    end_time = time()\n",
    "\n",
    "    run_time = end_time-begin_time\n",
    "\n",
    "    print(run_time)\n",
    "    \n",
    "    learning_rate.append(best['learning_rate'])\n",
    "    max_depth.append(int(best['max_depth']))\n",
    "    n_estimators.append(int(best['n_estimators']))\n",
    "    min_child_weight.append(best['min_child_weight'])\n",
    "    subsample.append(best['subsample'])\n",
    "    colsample_bytree.append(best['colsample_bytree'])\n",
    "    reg_lambda.append(best['reg_lambda'])\n",
    "    gamma.append(best['gamma'])\n",
    "    \n",
    "    param = {\n",
    "                 'max_depth':        int(best['max_depth']),\n",
    "                 'learning_rate':    best['learning_rate'],\n",
    "                 'n_estimators':     int(best['n_estimators']),  \n",
    "                 'min_child_weight': best['min_child_weight'],\n",
    "                 'subsample':        best['subsample'],\n",
    "                 'colsample_bytree': best['colsample_bytree'],\n",
    "                 'reg_lambda':       best['reg_lambda'],\n",
    "                 'gamma':            best['gamma'], \n",
    "                 \"random_state\":123456\n",
    "                }\n",
    "    x_xgbr = XGBR(objective='reg:squarederror',**param)\n",
    "    scores_x = cross_val_score(x_xgbr,data_feature,data_target_x,cv=5)\n",
    "    MCVA_x_temp = scores_x.mean()\n",
    "    MCVA_x_temp = np.around(MCVA_x_temp, decimals = 4)\n",
    "    MCVA_x.append(MCVA_x_temp)\n",
    "    \n",
    "    GC_x_temp = (1-scores_x.mean())**2+scores_x.var()\n",
    "    GC_x_temp = np.around(GC_x_temp, decimals = 4)\n",
    "    GC_x.append(GC_x_temp)\n",
    "    \n",
    "    x_xgbr.fit(Xtrain_x,Ytrain_x)\n",
    "\n",
    "    y_xgbr = XGBR(objective='reg:squarederror',**param)\n",
    "    scores_y = cross_val_score(y_xgbr,data_feature,data_target_y,cv=5)\n",
    "    MCVA_y_temp = scores_y.mean()\n",
    "    MCVA_y_temp = np.around(MCVA_y_temp, decimals = 4)\n",
    "    MCVA_y.append(MCVA_y_temp)\n",
    "    \n",
    "    GC_y_temp = (1-scores_y.mean())**2+scores_y.var()\n",
    "    GC_y_temp = np.around(GC_y_temp, decimals = 4)\n",
    "    GC_y.append(GC_y_temp)\n",
    "    \n",
    "    y_xgbr.fit(Xtrain_y,Ytrain_y)\n",
    "\n",
    "    multioutputregressor = MultiOutputRegressor(XGBR(objective='reg:squarederror',**param))\n",
    "    scores = cross_val_score(multioutputregressor,data_feature,data_target,cv=5)\n",
    "    MCVA_temp = scores.mean()\n",
    "    MCVA_temp = np.around(MCVA_temp, decimals = 4)\n",
    "    MCVA.append(MCVA_temp)\n",
    "    \n",
    "    GC_temp = (1-scores.mean())**2+scores.var()\n",
    "    GC_temp = np.around(GC_temp, decimals = 4)\n",
    "    GC.append(GC_temp)\n",
    "    \n",
    "    multioutputregressor.fit(Xtrain, Ytrain)\n",
    "\n",
    "    TS_temp = multioutputregressor.score(Xtest,Ytest)\n",
    "    TS_temp = np.around(TS_temp, decimals = 4)\n",
    "    TS.append(TS_temp)\n",
    "    # 对测点的数据进行验证\n",
    "    source_feature_all = ['wave_rate1','aver1','median1','fft_shape_mean1','fft_mean1','SamEn1',\n",
    "                   'wave_rate2','aver2','median2','fft_shape_mean2','fft_mean2','SamEn2',\n",
    "                   'wave_rate3','aver3','median3','fft_shape_mean3','fft_mean3','SamEn3',\n",
    "                   'wave_rate4','aver4','median4','fft_shape_mean4','fft_mean4','SamEn4',]\n",
    "    source = pd.read_csv(\"./Testing_datasets/test_data.csv\",index_col=0) # 12-40\n",
    "    for i in range(len(id_delete)):\n",
    "        source = source.drop(source_feature_all[id_delete[i]],axis=1)\n",
    "\n",
    "    Multi_pred_source=multioutputregressor.predict(source)\n",
    "    print(Multi_pred_source)\n",
    "    Multi_pred_source_x=Multi_pred_source[0,0]\n",
    "    Multi_pred_source_x = np.around(Multi_pred_source_x,decimals=4)\n",
    "    x_estimate.append(Multi_pred_source_x)\n",
    "    Multi_pred_source_y=Multi_pred_source[0,1]\n",
    "    Multi_pred_source_y = np.around(Multi_pred_source_y,decimals=4)\n",
    "    y_estimate.append(Multi_pred_source_y)\n",
    "    LE_temp = np.sqrt((Multi_pred_source_x - 4565)**2+(Multi_pred_source_y - 7621)**2)\n",
    "    LE_temp = np.around(LE_temp, decimals = 4)\n",
    "    LE.append(LE_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Inversion_results = {'MCVA':MCVA,\n",
    "                    'MCVA_x':MCVA_x,\n",
    "                    'MCVA_y':MCVA_y,\n",
    "                    'GC':GC,\n",
    "                    'GC_x':GC_x,\n",
    "                    'GC_y':GC_y,\n",
    "                    'TS': TS,\n",
    "                    'x':x_estimate,\n",
    "                    'y':y_estimate,\n",
    "                    'LE':LE,\n",
    "                    'max_depth':max_depth,\n",
    "                    'learning_rate':learning_rate,\n",
    "                    'n_estimators':n_estimators,\n",
    "                    'min_child_weight':min_child_weight,\n",
    "                    'subsample':subsample,\n",
    "                    'colsample_bytree':colsample_bytree,\n",
    "                    'reg_lambda':reg_lambda,\n",
    "                    'gamma':gamma\n",
    "                    }\n",
    "Inversion_results['x'] = np.around(Inversion_results['x'],decimals=4)\n",
    "Inversion_results['y'] = np.around(Inversion_results['y'],decimals=4)\n",
    "Inversion_results = pd.DataFrame(Inversion_results)\n",
    "Inversion_results.to_csv('./Reconstruction_results/inversion_Oct4_after_feature_selection_50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 置信区间统计\n",
    "from scipy import stats\n",
    "inversion_before_FS  = pd.read_csv('./Reconstruction_results/inversion_Oct4_before_feature_selection_50.csv',index_col=0)\n",
    "inversion_before_FS_x_aver = np.mean(inversion_before_FS['x'].values)\n",
    "inversion_before_FS_x_std = np.std(inversion_before_FS['x'].values)\n",
    "inversion_before_FS_x_low = inversion_before_FS_x_aver - 1.96 * inversion_before_FS_x_std\n",
    "inversion_before_FS_x_high = inversion_before_FS_x_aver + 1.96 * inversion_before_FS_x_std\n",
    "\n",
    "inversion_before_FS_y_aver = np.mean(inversion_before_FS['y'].values)\n",
    "inversion_before_FS_y_std = np.std(inversion_before_FS['y'].values)\n",
    "inversion_before_FS_y_low = inversion_before_FS_y_aver - 1.96 * inversion_before_FS_y_std\n",
    "inversion_before_FS_y_high = inversion_before_FS_y_aver + 1.96 * inversion_before_FS_y_std\n",
    "\n",
    "print(\"Before feature selection\")\n",
    "print(\"x interval: [\", inversion_before_FS_x_low, \",\" ,inversion_before_FS_x_high, \"]\")\n",
    "print(\"y interval: [\", inversion_before_FS_y_low, \",\" ,inversion_before_FS_y_high, \"]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inversion_after_FS  = pd.read_csv('./Reconstruction_results/inversion_Oct4_after_feature_selection_50.csv',index_col=0)\n",
    "inversion_after_FS_x_aver = np.mean(inversion_after_FS['x'].values)\n",
    "inversion_after_FS_x_std = np.std(inversion_after_FS['x'].values)\n",
    "inversion_after_FS_x_low = inversion_after_FS_x_aver - 1.96 * inversion_after_FS_x_std\n",
    "inversion_after_FS_x_high = inversion_after_FS_x_aver + 1.96 * inversion_after_FS_x_std\n",
    "\n",
    "inversion_after_FS_y_aver = np.mean(inversion_after_FS['y'].values)\n",
    "inversion_after_FS_y_std = np.std(inversion_after_FS['y'].values)\n",
    "inversion_after_FS_y_low = inversion_after_FS_y_aver - 1.96 * inversion_after_FS_y_std\n",
    "inversion_after_FS_y_high = inversion_after_FS_y_aver + 1.96 * inversion_after_FS_y_std\n",
    "\n",
    "print(\"After feature selection\")\n",
    "print(\"x interval: [\", inversion_after_FS_x_low, \",\" ,inversion_after_FS_x_high, \"]\")\n",
    "print(\"y interval: [\", inversion_after_FS_y_low, \",\" ,inversion_after_FS_y_high, \"]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 计算特征重要性\n",
    "source_feature_all = ['wave_rate1','aver1','median1','fft_shape_mean1','fft_mean1','SamEn1',\n",
    "                   'wave_rate2','aver2','median2','fft_shape_mean2','fft_mean2','SamEn2',\n",
    "                   'wave_rate3','aver3','median3','fft_shape_mean3','fft_mean3','SamEn3',\n",
    "                   'wave_rate4','aver4','median4','fft_shape_mean4','fft_mean4','SamEn4',]\n",
    "source_feature_all\n",
    "new_list = [source_feature_all[i] for i in range(len(source_feature_all)) if i not in id_delete]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_set = FontProperties(fname=r\"C:\\Windows\\Fonts\\simhei.ttf\", size=60)\n",
    "font_set2 = FontProperties(fname=r\"C:\\Windows\\Fonts\\simhei.ttf\", size=60)\n",
    "feature_importances0=multioutputregressor.estimators_[0].feature_importances_\n",
    "feature_importances1=multioutputregressor.estimators_[1].feature_importances_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ind = np.arange(len(new_list))\n",
    "width = 0.3\n",
    "rect1 = ax.barh(ind - width/2, feature_importances0,width,label='x',color='y')\n",
    "rect1 = ax.barh(ind + width/2, feature_importances1,width,label='y',color='purple')\n",
    "plt.tick_params(labelsize=30)\n",
    "ax.set_yticks(ind + width / 2)\n",
    "ax.set_yticklabels(new_list)\n",
    "ax.set_xlabel('Feature importance',fontsize = 35)\n",
    "ax.legend(fontsize=30)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"./Reconstruction_results/Feature_importance_Oct4.png\",dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
